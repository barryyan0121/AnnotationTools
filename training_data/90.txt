Ethan Hu  
 
Summary:  
      Several years of Data Scientist, Data Analyst experience with in-depth knowledge of business process in Healthcare, IT, Media, Investment and Finance industries. Extensive experience in transforming business requirement into analytical models, designing algorithms, and strategic solutions  
      Experienced in the entire data science project life cycle, including Business Understanding, ETL, Data Acquisition, Data Mining, Data Cleaning, Data Manipulation, EDA, Machine Learning, Model Selection, Algorithm Comparison and Deployment  
      Experienced with machine learning algorithm: Linear Regression, Logistic Regression, SVM, Decision Tree (CART, classification, and regression tree), Random Forest, K-Means Clustering, KNN, AdaBoost, XGBoost, Naïve Bayes, Neural Networks, Recommendation System Design  
      Skilled in deep learning algorithm such as CNN, RNN, LSTMs, Stacked Auto-Encoders, Deep Boltzmann Machine, Deep Belief Networks, Perceptron and MLP  
      Expert in Python 3.6 - Python 3.8 with Numpy, Pandas, Scipy, Scikit-Learn, Matplotlib, Seaborn, Plotly, AutoML, mlxtend, PySpark and deep learning packages such as Keras, Tensorflow, Theano, PyTorch, Microsoft Cognitive Toolkit (CNTK)  
      Skilled in using Grid Search and Random Search to implement Hyperparameter Tuning for Model Selection, skilled in Hyperopt  
      Skilled in Auto Machine Learning Systems such as Auto-Sklearn, MLBox, TPOT, H2O AutoML, Auto-WEKA, Hyperopt, Auto-Keras, TransmogrifAL and more  
      Expert in utilized Confusion Matrix, F1 Score, Recall, Precision to evaluate the performance of models  
      Extensive experience with version control tool Git in Bitbucket, GitLab and GitHub as well as its built-in Continuous Integration tool, Container tool Docker, cloud platform such as AWS (SageMaker, EC2, S3), Amazon Mechanical Turk and Google Cloud Platform  
      Skilled in collaboration tools like Agile, Azure DevOps, Confluence, Trello, Phabricator, and Jira  
      Excel at Natural Language Process (NLTK) and Text Preprocessing skills, including tokenization (Regular Expression), stemming, lemmatization, POS Tags, Named Entity Recognition, Chunking, stop words removal, TF-IDF  
      Strong skills in Statistics methodologies such as Hypothesis Testing, A/B Testing, ANOVA, Principal Component Analysis (PCA), ARIMA Forecasting  
      Solid ability to write and optimize diverse SQL Queries, working knowledge of RDBMS (MS SQL Server, IBM DB2, Oracle, MySQL, Microsoft Access), NoSQL (Amazon DynamoDB, Redis, MongoDB) and Graph Database (Neo4j)  
      Deep understanding of Data Visualization, building and publishing customized interactive reports (Bokeh, HoloViews, hvPlot and React packages (Semiotic, React Google Charts, D3)) and dashboards with customized parameters and user-filters using Tableau 9.4/9.2  
      Skilled in Hadoop ecosystem like HDFS, MapReduce, Spark, Hive, Kafka, Pig, Sqoop and Zookeeper  
      Excel at IDEs like Pycharm, Spyder, IDLE, Sublime Text 3, Visual Studio, Atom, Google CoLab and Jupyter  
      Effective team player. Advanced in Project Management, Complicated and Complex Presentations, Conceptual Innovation, Consulting, Cross-Functional Teamwork, Digital and New Media, Optimization, Product Development, Product Management, Service Line Development, Strategic Planning and Execution  
 
TECHNICAL SKILLS:  
 
Languages\ Operating Systems: \  
Python 3.6/3.7/3.8, SQL, R 3.4-3.6, Java 8/11, \ Windows 10/8/7/XP, Linux (Ubuntu, Debian, \  
React 15.6/16.0\ NixOS), macOS\  
 
Packages: \ Cloud Platform: \  
Python (Numpy, Pandas, Scikit-learn, Scipy \ Amazon Web Service (SageMaker, EC2, S3), \  
and Matplotlib, seaborn, NLTK, Spacy, \ Google Cloud Platform, Azure DevOps, Jira \  
gensim), R (caret, dplyr, Glmnet, lavaan, \ 8.1.0, MTurk, Agile, Confluence, Trello\  
bnlearn, ggplot2, GoogleVis, Shiny) \  
 
Database: \ BI Tools: \  
MySQL 5.5, PostgreSQL 9/10/11, MongoDB, \ Tableau 9.4/9.2, Superset, MS Office (Word/ \  
Microsoft SQL Server/ MSSQL 2017, \ Excel/PowerPoint/Visio) \  
Dynamo DB, Redis\  
 
Professional Experience:  
 
Marlabs Inc, PISCATAWAY, NJ Jul 2020 - Till Date  
Client: Gannett Healthcare Group  
Project: Open Payments Data Analysis  
Role: Data Scientist  
 
Project Description:  
Gannett Healthcare Group was designed to improve the quality, stability, and efficiency of healthcare services. Open Payments is a national transparency program that collects and publishes information about financial relationships between the health care industry and providers. Based on Gannett Healthcare Group's interests, perform data science process on Top 100 physicians related to specific drugs and build machine learning models to predict the payments of those physicians for the next 5 years and predict the possibility of a particular drug being successfully launched on the market.  
 
Responsibilities:  
      Used JupyterLab in AWS SageMaker as Coding environment  
      Used Pandas, Numpy, Sodapy, tqdm, matplotlib, sklearn and math package  
      Built an API package in Python to scrape information from https://openpaymentsdata.cms.gov/ by using Socrata Token  
      Implemented Exploratory Data Analysis (EDA) by Year, payments by Type, Payments by Entity, Recipients, Physicians, Region and Amount using Tableau 9.4  
      Started with baseline model using global mean payments and user-averaged payments for payments prediction and physician recommendation  
      Applied Auto-sklearn for automatic feature selection, model selection, Hyperparameter Tuning and algorithm comparison  
      Utilized Mean Squared Error, F1 score, AUC, and Confusion Matrix to evaluate the performance of models  
      Created core group and filter functions by physician, product list, recipients and Amounts for all teammates use  
    Models are deployed using AWS SageMaker and stored in AWS S3 bucket  
      Created maps in python by using gmaps package for location of Recipients, Physicians and Payments Amounts  
      Used Jira 8.1.0 for issue tracking and project management  
 
Environment:  
Python 3.8, AWS (SageMaker/EC2/S3), Tableau 9.4, Git 2.24.3, Jira 8.1.0, Agile, JupyterLab  
 
 
Apothecom, New York City, NY Jan 2020 - May 2020  
Role: Data Scientist/Data Analyst  
 
Project Description:  
Apothecom is a pharmaceutical, treatment process and medical publication company. The project is to work with the Vice President of Analytics and Insights, perform several key responsibilities within the DnA team, including competitive intelligence, congress and publication activities and other miscellaneous assignments. This project is including data integration, clean-up and data visualization for data descriptive analysis; then through modeling and parameterization, predictive analysis of customer data, ultimately improve the efficiency of the pharmaceutical or treatment process, and enhance the company's profitability. The mainly focus of the project is to perform the NLP process of over 100 thousand scientific journal manuscripts data from National Center for Biotechnology Information (NCBI) to predict the manuscripts most likely to be published in top journals within some certain period.  
 
Responsibilities:  
 
      Utilized NLP Natural Language Process, Tensorflow, Machine Learning algorithm such as Random Forest, KNN, Boosted Tree, Deep Learning algorithm  
      Utilized Data Visualization tools such as Excel Charts, Tableau, Matplotlib, Seaborn and Plotly packages to analyze literature data and explore the intrinsic connections and business opportunities in medical publication field.  
      Acquired and coordinated over 5,000,000 data records from 15 different online data sources such as NCBI, PubMed, Netbase, Symplur, Twitter API etc, Automatically extract article keywords, title, abstract, author, publication time, publication magazine, type, and cited information  
      Built pipelines of cleaning, updating, and inserting data into the database by using Python programming (pandas and numpy)  
      Used Sodapy, tqdm, matplotlib, sklearn, Bio, Entrez, Medline, Requests, Json, Time, Datetime, bs4, BeautifulSoup, urllib, xlsxwriter, nltk, calendar, string, pickle, ibm-watson, and math package  
      Applied Sentiment Analysis finding relationships between healthcare providers and patients and find evaluations of the latest drug development process, ensure that the company grasps the latest developments in drug research at the first time  
      Applied text clustering algorithm, LSTM and LDA algorithm  
      Applied tokenization, lemmatization and stemming  
      Utilized Microsoft Visio to facilitate the visual processing, analysis and communication of complex information, systems, and processes  
      Used JupyterLab as Coding environment  
    Created GitHub workflow for daily execution  
      Utilized Agile and Jira 8.1.0 for planning, issue tracking and project management  
 
Environment:  
Python 3.7, Microsoft PowerPoint 2019, Microsoft Visio 2019, Git 2.24.3, Agile, Jira 8.1.0, JupyterLab, Tableau 2020.2.5, NLP  
 
 
Grove Property Fund, Hartford, CT Jun 2019 - Dec 2019  
Role: Data scientist  
 
Project Description:  
Grove Property Fund LLC is a private investor of commercial real estate looking to invest and allocate $15 Million in the Charlotte area of North Carolina. An analytics approach was used to predict the market value of a property after five years of its purchase date. A qualitative approach was used to gather information to study the market in Charlotte, NC such as population growth, crime, household income, school district and government investment in infrastructure. A quantitative approach was used to identify the important variables and construct a random forest model to predict the market value in five years of properties listed for sale. Six properties were identified using parameters established in the property selection methodology.  
 
Responsibilities:  
      Identified $15,000,000 of properties that have a combination of good access to highway, public transportation, the positive trend of growing population and income, lower than market price, high density and traffic, Found the 8 most worthy investment properties in the Charlotte area  
      Retrieved two main datasets from CoStar including more than 10,000 data points.  
      Trained classification models of Logistic Regression, Support Vector Machine (SVM), Random Forest and Gaussian Naive Bayes in JupyterLab  
      Developed an advanced model and Tableau Dashboard that can be updated and support users identifying potential target properties for future use  
      Utilized LASSO, RIDGE and ELASTIC NET regularization to avoid over-fitting problem  
      Utilized Python packages of Numpy, Pandas, Scipy, Scikit-Learn, Matplotlib, Seaborn, Plotly  
 
Environment:  
Microsoft Visio, Microsoft Excel, Python 3.7, Microsoft PowerPoint, Microsoft Visio, Git 2.24.3  
 
 
Wuchan Tonghe, Hangzhou, China Jan 2018 - Dec 2018  
Role: Investment Data Scientist Intern  
 
Project Description:  
The business scope of Wuchan Tonghe includes general business items: investment management. The project is focusing on identifying company's stock price, predicting stock price, charged-off or defaults and supporting investment decision making.  
 
Responsibilities:  
      Extracted, loaded, and transformed (ETL) data in the database using MySQL and Python programming  
      Defined data type to reduce memory usage before processing the dataset to improve efficiency  
      Performed EDA in Python 3.6 to segment different company type into different groups based on the business scope  
      Preprocessed dataset by feature engineering, Categorical feature Encoder, normalization and conducted SMOTE sampling to increase the percentage of the minority class  
      Utilized time series analysis [Regression, Autoregressive Integrated Moving Average (ARIMA) models etc.] to forecast price and formulate investment strategies  
      Created a 200-page report on the analysis of new energy vehicles (TESLA) to integrate the information of the upstream and downstream industry chains  
      Conducted data analysis by visualizing data such as the distribution of Stock Price, Market Cap, Volume, and PE Ratio in MS Excel 2013  
 
Environment:  
Python 3.6, MS Office 2013 (Excel/ PowerPoint/ Word), SAS  
 
Lianxiang, Hangzhou, Zhejiang, China Sep 2016 - Nov 2017  
Role: Marketing Data Analyst Intern  
 
Project Description:  
Lenovo designs, develops, manufactures, and sells personal computers, tablet computers, smartphones, workstations, servers, electronic storage devices, IT management software, and smart televisions, and is the world's largest personal computer vendor by unit sales as of March 2019. The company hopes to formulate a sales strategy for the new computer Lenovo Rescue and Lenovo Xiaoxin in Southeast China. This project provides data support for this overall sales strategy.  
 
Responsibilities:  
      Performed SQL queries to manage database and extract data from database to directly construct features for high efficiency  
      Get data from websites, used MSSQL 2017 to build collected data  
      Visualized desktop sales data in Microsoft Office Excel and Python 3.6 such as bar chart, pie chart, bubble chart for different dimensions of features including Desktop type, Sales and Price, stacked bar chart for home size and geo chart for crime rates  
      Set data type for features to reduce memory usage when creating data frame in Python 3.6  
      Applied variable selection, divided dataset to training data, testing data and validation data, labeled desktop variable to profitable and non-profitable  
      Collected [local sales, costs, profits, consumer age structure etc.] over 120,000 data records utilizing Excel, executed data processing, cleaning and analysis using Python. Led a group of six interns, participated in the National Intern Conference, reported on the research results, and won the second prize  
 
Environment:  
Python 3.6, MS Office 2013 (Excel/ PowerPoint/ Word), R  
 
Yunshu, Hangzhou, China Nov 2014 - Aug 2016  
Role: Data Analyst Intern  
 
Project Description:  
Yunshu is a project dedicated to the recycling of free second-hand textbooks in colleges and universities and doing its best to protect the environment. This project is analyzing the type of different textbooks and store the textbook data into company's database.  
Responsibilities:  
 
         Extracted data and other information from PDFs  
         Performed SQL queries to extract book data from MySQL database  
         Designed User Interfaces for the decision-maker, including mechanisms to enter commands, make requests, enter data, access methods to generate outputs  
         Applied User-Forms including Controls, Combo Boxes, Option Buttons, Scroll Bar, List Boxes and Excel Formulas to enhance user interfaces in VBA  
         Created API for extracting data from database  
         Conducted Exploratory Data Analysis for network analysis  
 
Environment:  
Python 3.6, MS Office 2013 (Excel/ PowerPoint/ Word), SQL, MySQL  
 
Education  
 
Master of Science, Business Analytics & Project MNGT  
GPA: 3.96/4.00  
 
 
 
Study Abroad, Bachelor of Science  
 
Completed coursework in Economics, Finance and Bioscience  
 
 
 
 
 
 
 
 
 
 
 
 
Page 3 of 6  
1 Corporate Place South, Piscataway NJ 08854  
 
 
 
 



